{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/wejarrard/Library/Application Support/sagemaker/config.yaml\n",
      "Deleted all objects in sagemaker-us-west-2-016114370410/tf-binding-sites/inference/input\n",
      "Input spec: s3://sagemaker-us-west-2-016114370410/tf-binding-sites/inference/input\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Initialize a SageMaker session\n",
    "sagemaker_session = Session()\n",
    "\n",
    "role = \"arn:aws:iam::016114370410:role/tf-binding-sites\"\n",
    "\n",
    "prefix = \"tf-binding-sites/inference/input\"\n",
    "local_dir = \"/Users/wejarrard/projects/tf-binding/data/jsonl\"\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your S3 bucket name\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "# Function to delete all objects in a specified S3 bucket/prefix\n",
    "def delete_s3_objects(bucket_name, prefix):\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if 'Contents' in response:\n",
    "        for item in response['Contents']:\n",
    "            s3.delete_object(Bucket=bucket_name, Key=item['Key'])\n",
    "        print(f\"Deleted all objects in {bucket_name}/{prefix}\")\n",
    "    else:\n",
    "        print(f\"No objects found in {bucket_name}/{prefix} to delete.\")\n",
    "\n",
    "# Delete existing files from the specified S3 location\n",
    "delete_s3_objects(bucket_name, prefix)\n",
    "\n",
    "# Upload new files to the specified S3 location\n",
    "inputs = sagemaker_session.upload_data(path=local_dir, key_prefix=prefix)\n",
    "print(f\"Input spec: {inputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get model artifact location by estimator.model_data, or give an S3 key directly\n",
    "model_artifact_s3_location = \"s3://tf-binding-sites/finetuning/results/output/AR-LOO-THP-1-2024-05-15-00-00-49-482/output/model.tar.gz\"\n",
    "\n",
    "# Create PyTorchModel from saved model artifact\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artifact_s3_location,\n",
    "    role=role,\n",
    "    framework_version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    # image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.1.0-gpu-py310-cu118-ubuntu20.04-sagemaker-v1.8\",\n",
    "    source_dir=\"/Users/wejarrard/projects/tf-binding/src/inference/scripts\",\n",
    "    entry_point=\"inference.py\",\n",
    "    # code_location=\"inference/code\",   \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create transformer from PyTorchModel object\n",
    "output_path = f\"s3://tf-binding-sites/inference/output/\"\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1, \n",
    "                                        instance_type=\"ml.g5.2xlarge\", \n",
    "                                        output_path=output_path,\n",
    "                                        strategy=\"MultiRecord\",\n",
    "                                        max_concurrent_transforms=1,\n",
    "                                        max_payload=100,\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: pytorch-inference-2024-06-12-23-05-54-065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation output saved to: s3://tf-binding-sites/inference/output/\n"
     ]
    }
   ],
   "source": [
    "# Start the transform job\n",
    "transformer.transform(\n",
    "    data=inputs,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/jsonlines\",\n",
    "    split_type=\"Line\",\n",
    "    # compression_type=\"Gzip\",\n",
    "    wait=False,\n",
    ")\n",
    "\n",
    "print(f\"Transformation output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
