{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted all objects in sagemaker-us-west-2-016114370410/tf-binding-sites/inference/input\n",
      "Input spec: s3://sagemaker-us-west-2-016114370410/tf-binding-sites/inference/input\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Initialize a SageMaker session\n",
    "sagemaker_session = Session()\n",
    "\n",
    "role = \"arn:aws:iam::016114370410:role/tf-binding-sites\"\n",
    "\n",
    "prefix = \"tf-binding-sites/inference/input\"\n",
    "local_dir = \"/Users/wejarrard/projects/tf-binding/data/jsonl\"\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your S3 bucket name\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "# Function to delete all objects in a specified S3 bucket/prefix\n",
    "def delete_s3_objects(bucket_name, prefix=\"\"):\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    if 'Contents' in response:\n",
    "        for item in response['Contents']:\n",
    "            s3.delete_object(Bucket=bucket_name, Key=item['Key'])\n",
    "        print(f\"Deleted all objects in {bucket_name}/{prefix}\")\n",
    "    else:\n",
    "        print(f\"No objects found in {bucket_name}/{prefix} to delete.\")\n",
    "\n",
    "# Delete existing files from the specified S3 location\n",
    "delete_s3_objects(bucket_name, prefix)\n",
    "delete_s3_objects(bucket_name=\"s3://tf-binding-sites\", prefix=\"inference/output\")\n",
    "\n",
    "# Upload new files to the specified S3 location\n",
    "inputs = sagemaker_session.upload_data(path=local_dir, key_prefix=prefix)\n",
    "print(f\"Input spec: {inputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://tf-binding-sites/finetuning/results/output/AR-LOO-THP-1-2024-05-15-00-00-49-482/output/model.tar.gz), script artifact (/Users/wejarrard/projects/tf-binding/src/inference/scripts), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-west-2-016114370410/pytorch-inference-2024-06-27-16-10-52-354/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2024-06-27-16-13-46-035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get model artifact location by estimator.model_data, or give an S3 key directly\n",
    "model_artifact_s3_location = \"s3://tf-binding-sites/finetuning/results/output/AR-LOO-THP-1-2024-05-15-00-00-49-482/output/model.tar.gz\"\n",
    "\n",
    "# Create PyTorchModel from saved model artifact\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artifact_s3_location,\n",
    "    role=role,\n",
    "    framework_version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    # image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.1.0-gpu-py310-cu118-ubuntu20.04-sagemaker-v1.8\",\n",
    "    source_dir=\"/Users/wejarrard/projects/tf-binding/src/inference/scripts\",\n",
    "    entry_point=\"inference.py\",\n",
    "    # code_location=\"inference/code\",   \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create transformer from PyTorchModel object\n",
    "output_path = f\"s3://tf-binding-sites/inference/output/\"\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1, \n",
    "                                        instance_type=\"ml.g5.2xlarge\", \n",
    "                                        output_path=output_path,\n",
    "                                        strategy=\"MultiRecord\",\n",
    "                                        max_concurrent_transforms=1,\n",
    "                                        max_payload=100,\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: pytorch-inference-2024-06-27-16-13-47-776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................Collecting accelerate==0.31.0 (from -r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting certifi==2024.6.2 (from -r /opt/ml/model/code/requirements.txt (line 2))\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 3)) (3.3.2)\n",
      "Collecting discrete-key-value-bottleneck-pytorch==0.1.1 (from -r /opt/ml/model/code/requirements.txt (line 4))\n",
      "  Downloading discrete_key_value_bottleneck_pytorch-0.1.1-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting einops==0.8.0 (from -r /opt/ml/model/code/requirements.txt (line 5))\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting einx==0.3.0 (from -r /opt/ml/model/code/requirements.txt (line 6))\n",
      "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting enformer-pytorch==0.8.8 (from -r /opt/ml/model/code/requirements.txt (line 7))\n",
      "  Downloading enformer_pytorch-0.8.8-py3-none-any.whl.metadata (863 bytes)\n",
      "Requirement already satisfied: filelock==3.14.0 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 8)) (3.14.0)\n",
      "Collecting frozendict==2.4.4 (from -r /opt/ml/model/code/requirements.txt (line 9))\n",
      "  Downloading frozendict-2.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: fsspec==2024.6.0 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 10)) (2024.6.0)\n",
      "Collecting huggingface-hub==0.23.3 (from -r /opt/ml/model/code/requirements.txt (line 11))\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: idna==3.7 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 12)) (3.7)\n",
      "Collecting importlib_metadata==7.1.0 (from -r /opt/ml/model/code/requirements.txt (line 13))\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 14)) (3.1.4)\n",
      "Collecting lightning-utilities==0.11.2 (from -r /opt/ml/model/code/requirements.txt (line 15))\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe==2.1.5 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 16)) (2.1.5)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 17)) (1.3.0)\n",
      "Requirement already satisfied: networkx==3.3 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 18)) (3.3)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 19)) (1.26.4)\n",
      "Collecting packaging==24.1 (from -r /opt/ml/model/code/requirements.txt (line 20))\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas==2.2.2 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 21)) (2.2.2)\n",
      "Collecting polars==0.20.31 (from -r /opt/ml/model/code/requirements.txt (line 22))\n",
      "  Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: psutil==5.9.8 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 23)) (5.9.8)\n",
      "Collecting pyfaidx==0.8.1.1 (from -r /opt/ml/model/code/requirements.txt (line 24))\n",
      "  Downloading pyfaidx-0.8.1.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pysam==0.22.1 (from -r /opt/ml/model/code/requirements.txt (line 25))\n",
      "  Downloading pysam-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting python-dateutil==2.9.0.post0 (from -r /opt/ml/model/code/requirements.txt (line 26))\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pytz==2024.1 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 27)) (2024.1)\n",
      "Collecting PyYAML==6.0.1 (from -r /opt/ml/model/code/requirements.txt (line 28))\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex==2024.5.15 (from -r /opt/ml/model/code/requirements.txt (line 29))\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests==2.32.3 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 30)) (2.32.3)\n",
      "Collecting safetensors==0.4.3 (from -r /opt/ml/model/code/requirements.txt (line 31))\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 32)) (1.16.0)\n",
      "Requirement already satisfied: sympy==1.12.1 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 33)) (1.12.1)\n",
      "Collecting tokenizers==0.19.1 (from -r /opt/ml/model/code/requirements.txt (line 34))\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: torch==2.1.0 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 35)) (2.1.0+cu118)\n",
      "Collecting torchmetrics==1.4.0.post0 (from -r /opt/ml/model/code/requirements.txt (line 36))\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tqdm==4.66.4 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 37)) (4.66.4)\n",
      "Collecting transformers==4.41.2 (from -r /opt/ml/model/code/requirements.txt (line 38))\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 12.8 MB/s eta 0:00:00\n",
      "Collecting typing_extensions==4.12.2 (from -r /opt/ml/model/code/requirements.txt (line 39))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: tzdata==2024.1 in /opt/conda/lib/python3.10/site-packages (from -r /opt/ml/model/code/requirements.txt (line 40)) (2024.1)\n",
      "Collecting urllib3==2.2.1 (from -r /opt/ml/model/code/requirements.txt (line 41))\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting vector-quantize-pytorch==1.14.24 (from -r /opt/ml/model/code/requirements.txt (line 42))\n",
      "  Downloading vector_quantize_pytorch-1.14.24-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting zipp==3.19.2 (from -r /opt/ml/model/code/requirements.txt (line 43))\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities==0.11.2->-r /opt/ml/model/code/requirements.txt (line 15)) (65.6.3)\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 309.4/309.4 kB 13.7 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.4/164.4 kB 77.7 MB/s eta 0:00:00\n",
      "Downloading discrete_key_value_bottleneck_pytorch-0.1.1-py3-none-any.whl (3.9 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.2/43.2 kB 24.0 MB/s eta 0:00:00\n",
      "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.0/103.0 kB 55.6 MB/s eta 0:00:00\n",
      "Downloading enformer_pytorch-0.8.8-py3-none-any.whl (96 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.0/97.0 kB 54.1 MB/s eta 0:00:00\n",
      "Downloading frozendict-2.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.3/117.3 kB 64.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 80.5 MB/s eta 0:00:00\n",
      "Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 30.0 MB/s eta 0:00:00\n",
      "Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28.8/28.8 MB 139.2 MB/s eta 0:00:00\n",
      "Downloading pyfaidx-0.8.1.1-py3-none-any.whl (28 kB)\n",
      "Downloading pysam-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl (22.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.0/22.0 MB 148.7 MB/s eta 0:00:00\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 95.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 705.5/705.5 kB 147.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 153.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 164.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 164.4 MB/s eta 0:00:00\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 868.8/868.8 kB 158.3 MB/s eta 0:00:00\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 165.4 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 64.0 MB/s eta 0:00:00\n",
      "Downloading vector_quantize_pytorch-1.14.24-py3-none-any.whl (36 kB)\n",
      "Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Installing collected packages: zipp, urllib3, typing_extensions, safetensors, regex, PyYAML, python-dateutil, pysam, polars, packaging, frozendict, einops, certifi, lightning-utilities, importlib_metadata, einx, vector-quantize-pytorch, torchmetrics, pyfaidx, huggingface-hub, tokenizers, discrete-key-value-bottleneck-pytorch, accelerate, transformers, enformer-pytorch\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.19\n",
      "    Uninstalling urllib3-1.26.19:\n",
      "      Successfully uninstalled urllib3-1.26.19\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.1\n",
      "    Uninstalling typing_extensions-4.12.1:\n",
      "      Successfully uninstalled typing_extensions-4.12.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0\n",
      "    Uninstalling python-dateutil-2.9.0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Successfully installed PyYAML-6.0.1 accelerate-0.31.0 certifi-2024.6.2 discrete-key-value-bottleneck-pytorch-0.1.1 einops-0.8.0 einx-0.3.0 enformer-pytorch-0.8.8 frozendict-2.4.4 huggingface-hub-0.23.3 importlib_metadata-7.1.0 lightning-utilities-0.11.2 packaging-24.1 polars-0.20.31 pyfaidx-0.8.1.1 pysam-0.22.1 python-dateutil-2.9.0.post0 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 torchmetrics-1.4.0.post0 transformers-4.41.2 typing_extensions-4.12.2 urllib3-2.2.1 vector-quantize-pytorch-1.14.24 zipp-3.19.2\n",
      "botocore 1.31.85 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Warning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2024-06-27T16:20:07,340 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
      "2024-06-27T16:20:07,342 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "2024-06-27T16:20:07,393 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "2024-06-27T16:20:07,464 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "Torchserve version: 0.11.0\n",
      "TS Home: /opt/conda/lib/python3.10/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /home/model-server/tmp\n",
      "Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "Number of GPUs: 1\n",
      "Number of CPUs: 8\n",
      "Max heap size: 7952 M\n",
      "Python executable: /opt/conda/bin/python3.10\n",
      "Config file: /etc/sagemaker-ts.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Metrics address: http://127.0.0.1:8082\n",
      "Model Store: /.sagemaker/ts/models\n",
      "Initial Models: model=/opt/ml/model\n",
      "Log dir: /logs\n",
      "Metrics dir: /logs\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 1\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Limit Maximum Image Pixels: true\n",
      "Prefer direct buffer: false\n",
      "Allowed Urls: [file://.*|http(s)?://.*]\n",
      "Custom python dependency for model allowed: false\n",
      "Enable metrics API: true\n",
      "Metrics mode: LOG\n",
      "Disable system metrics: true\n",
      "Workflow Store: /.sagemaker/ts/models\n",
      "CPP log config: N/A\n",
      "Model config: N/A\n",
      "System metrics command: default\n",
      "2024-06-27T16:20:07,470 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "2024-06-27T16:20:07,486 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "2024-06-27T16:20:07,493 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createTempDir /home/model-server/tmp/models/22808d860a024e749ab5fdbcf71d40d2\n",
      "2024-06-27T16:20:07,493 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createSymbolicDir /home/model-server/tmp/models/22808d860a024e749ab5fdbcf71d40d2/model\n",
      "2024-06-27T16:20:07,494 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "2024-06-27T16:20:07,494 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "2024-06-27T16:20:07,496 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "2024-06-27T16:20:07,503 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2024-06-27T16:20:07,561 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "2024-06-27T16:20:07,561 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "2024-06-27T16:20:07,562 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "2024-06-27T16:20:07,694 [INFO ] pool-2-thread-2 ACCESS_LOG - /169.254.255.130:55080 \"GET /ping HTTP/1.1\" 200 10\n",
      "2024-06-27T16:20:07,695 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505207\n",
      "2024-06-27T16:20:07,711 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:55082 \"GET /execution-parameters HTTP/1.1\" 404 3\n",
      "2024-06-27T16:20:07,712 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505207\n",
      "Model server started.\n",
      "2024-06-27T16:20:07,899 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:5e57ef1e7ff0,timestamp:1719505207\n",
      "2024-06-27T16:20:08,870 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=81\n",
      "2024-06-27T16:20:08,871 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "2024-06-27T16:20:08,923 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\n",
      "2024-06-27T16:20:08,923 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]81\n",
      "2024-06-27T16:20:08,923 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "2024-06-27T16:20:08,924 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\n",
      "2024-06-27T16:20:08,926 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "2024-06-27T16:20:08,931 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "2024-06-27T16:20:08,934 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1719505208934\n",
      "2024-06-27T16:20:08,943 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "2024-06-27T16:20:07.722:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=100, BatchStrategy=MULTI_RECORD\n",
      "2024-06-27T16:20:12,956 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Model loaded successfully.\n",
      "2024-06-27T16:20:12,956 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Model loaded successfully.\n",
      "2024-06-27T16:20:12,961 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4027\n",
      "2024-06-27T16:20:12,961 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5461.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505212\n",
      "2024-06-27T16:20:12,962 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505212\n",
      "2024-06-27T16:20:12,962 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1719505212962\n",
      "2024-06-27T16:20:12,965 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1719505212\n",
      "2024-06-27T16:20:12,965 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Reading JSONLines dataset\n",
      "2024-06-27T16:20:12,966 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Reading JSONLines dataset\n",
      "2024-06-27T16:20:12,956 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Model loaded successfully.\n",
      "2024-06-27T16:20:12,956 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Model loaded successfully.\n",
      "2024-06-27T16:20:12,961 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4027\n",
      "2024-06-27T16:20:12,961 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5461.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505212\n",
      "2024-06-27T16:20:12,962 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505212\n",
      "2024-06-27T16:20:12,962 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1719505212962\n",
      "2024-06-27T16:20:12,965 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1719505212\n",
      "2024-06-27T16:20:12,965 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Reading JSONLines dataset\n",
      "2024-06-27T16:20:12,966 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Reading JSONLines dataset\n",
      "2024-06-27T16:20:19,238 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 50 batches.\n",
      "2024-06-27T16:20:19,238 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 50 batches.\n",
      "2024-06-27T16:20:19,238 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 50 batches.\n",
      "2024-06-27T16:20:19,238 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 50 batches.\n",
      "2024-06-27T16:20:20,328 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 100 batches.\n",
      "2024-06-27T16:20:20,329 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 100 batches.\n",
      "2024-06-27T16:20:20,328 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 100 batches.\n",
      "2024-06-27T16:20:20,329 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 100 batches.\n",
      "2024-06-27T16:20:21,419 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 150 batches.\n",
      "2024-06-27T16:20:21,419 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 150 batches.\n",
      "2024-06-27T16:20:21,419 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 150 batches.\n",
      "2024-06-27T16:20:21,419 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 150 batches.\n",
      "2024-06-27T16:20:22,511 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 200 batches.\n",
      "2024-06-27T16:20:22,511 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 200 batches.\n",
      "2024-06-27T16:20:22,511 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 200 batches.\n",
      "2024-06-27T16:20:22,511 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 200 batches.\n",
      "2024-06-27T16:20:24,698 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 300 batches.\n",
      "2024-06-27T16:20:24,698 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 300 batches.\n",
      "2024-06-27T16:20:24,698 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 300 batches.\n",
      "2024-06-27T16:20:24,698 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 300 batches.\n",
      "2024-06-27T16:20:25,792 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 350 batches.\n",
      "2024-06-27T16:20:25,792 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 350 batches.\n",
      "2024-06-27T16:20:25,792 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 350 batches.\n",
      "2024-06-27T16:20:25,792 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 350 batches.\n",
      "2024-06-27T16:20:26,887 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 400 batches.\n",
      "2024-06-27T16:20:26,887 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 400 batches.\n",
      "2024-06-27T16:20:26,887 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 400 batches.\n",
      "2024-06-27T16:20:26,887 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 400 batches.\n",
      "2024-06-27T16:20:27,981 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 450 batches.\n",
      "2024-06-27T16:20:27,982 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 450 batches.\n",
      "2024-06-27T16:20:27,981 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 450 batches.\n",
      "2024-06-27T16:20:27,982 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 450 batches.\n",
      "2024-06-27T16:20:29,077 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 500 batches.\n",
      "2024-06-27T16:20:29,077 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 500 batches.\n",
      "2024-06-27T16:20:29,077 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 500 batches.\n",
      "2024-06-27T16:20:29,077 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 500 batches.\n",
      "2024-06-27T16:20:30,173 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 550 batches.\n",
      "2024-06-27T16:20:30,173 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 550 batches.\n",
      "2024-06-27T16:20:30,174 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 550 batches.\n",
      "2024-06-27T16:20:30,221 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:17256.07|#ModelName:model,Level:Model|#type:GAUGE|#hostname:5e57ef1e7ff0,1719505230,92e0b118-521e-49f6-8cb2-aef88f226ac2, pattern=[METRICS]\n",
      "2024-06-27T16:20:30,222 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:17256.07|#ModelName:model,Level:Model|#hostname:5e57ef1e7ff0,requestID:92e0b118-521e-49f6-8cb2-aef88f226ac2,timestamp:1719505230\n",
      "2024-06-27T16:20:30,223 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:55092 \"POST /invocations HTTP/1.1\" 200 22331\n",
      "2024-06-27T16:20:30,223 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:2.2322173766E7|#model_name:model,model_version:default|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:5061706.036|#model_name:model,model_version:default|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:5061.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 17260\n",
      "2024-06-27T16:20:30,225 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,174 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processed 550 batches.\n",
      "2024-06-27T16:20:30,221 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:17256.07|#ModelName:model,Level:Model|#type:GAUGE|#hostname:5e57ef1e7ff0,1719505230,92e0b118-521e-49f6-8cb2-aef88f226ac2, pattern=[METRICS]\n",
      "2024-06-27T16:20:30,222 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:17256.07|#ModelName:model,Level:Model|#hostname:5e57ef1e7ff0,requestID:92e0b118-521e-49f6-8cb2-aef88f226ac2,timestamp:1719505230\n",
      "2024-06-27T16:20:30,223 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:55092 \"POST /invocations HTTP/1.1\" 200 22331\n",
      "2024-06-27T16:20:30,223 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:2.2322173766E7|#model_name:model,model_version:default|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:5061706.036|#model_name:model,model_version:default|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:5061.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "2024-06-27T16:20:30,224 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 17260\n",
      "2024-06-27T16:20:30,225 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:5e57ef1e7ff0,timestamp:1719505230\n",
      "\n",
      "Transformation output saved to: s3://tf-binding-sites/inference/output/\n"
     ]
    }
   ],
   "source": [
    "# Start the transform job\n",
    "transformer.transform(\n",
    "    data=inputs,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/jsonlines\",\n",
    "    split_type=\"None\",\n",
    "    # compression_type=\"Gzip\",\n",
    "    # wait=False,\n",
    ")\n",
    "\n",
    "print(f\"Transformation output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
