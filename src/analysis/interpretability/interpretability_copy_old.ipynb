{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time\n",
      "\u001b[1;31m[I 2025-03-18 20:38:13.270 ServerApp] Extension package jupyter_lsp took 1.5731s to import\n",
      "\u001b[1;31m[I 2025-03-18 20:38:13.696 ServerApp] Extension package jupyter_server_terminals took 0.4245s to import\n",
      "\u001b[1;31m[I 2025-03-18 20:38:20.414 ServerApp] jupyter_lsp | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:20.633 ServerApp] jupyter_server_terminals | extension was successfully linked.\n",
      "\u001b[1;31m[W 2025-03-18 20:38:20.640 LabApp] 'iopub_data_rate_limit' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "\u001b[1;31m[W 2025-03-18 20:38:20.651 ServerApp] ServerApp.iopub_data_rate_limit config is deprecated in 2.0. Use ZMQChannelsWebsocketConnection.iopub_data_rate_limit.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:20.651 ServerApp] jupyterlab | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:20.665 ServerApp] notebook | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:34.853 ServerApp] notebook_shim | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:35.617 ServerApp] notebook_shim | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:35.827 ServerApp] jupyter_lsp | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:35.831 ServerApp] jupyter_server_terminals | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.257 LabApp] JupyterLab extension loaded from /data1/home/rreid/miniconda3/envs/processing/lib/python3.10/site-packages/jupyterlab\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.257 LabApp] JupyterLab application directory is /data1/home/rreid/miniconda3/envs/processing/share/jupyter/lab\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.260 LabApp] Extension Manager is 'pypi'.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.284 ServerApp] jupyterlab | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.297 ServerApp] notebook | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.297 ServerApp] The port 8888 is already in use, trying another port.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.298 ServerApp] The port 8889 is already in use, trying another port.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.298 ServerApp] The port 8890 is already in use, trying another port.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.298 ServerApp] The port 8891 is already in use, trying another port.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.298 ServerApp] The port 8892 is already in use, trying another port.\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.299 ServerApp] Serving notebooks from local directory: /data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.299 ServerApp] Jupyter Server 2.14.1 is running at:\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.299 ServerApp] http://localhost:8822/tree?token=7f56cf546c22f632d09e21b9cfae1abc4eede24ef6e24f38\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.299 ServerApp]     http://127.0.0.1:8822/tree?token=7f56cf546c22f632d09e21b9cfae1abc4eede24ef6e24f38\n",
      "\u001b[1;31m[I 2025-03-18 20:38:36.299 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "\u001b[1;31m[C 2025-03-18 20:38:36.426 ServerApp] \n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    To access the server, open this file in a browser:\n",
      "\u001b[1;31m        file:///data1/home/rreid/.local/share/jupyter/runtime/jpserver-40768-open.html\n",
      "\u001b[1;31m    Or copy and paste one of these URLs:\n",
      "\u001b[1;31m        http://localhost:8822/tree?token=7f56cf546c22f632d09e21b9cfae1abc4eede24ef6e24f38\n",
      "\u001b[1;31m        http://127.0.0.1:8822/tree?token=7f56cf546c22f632d09e21b9cfae1abc4eede24ef6e24f38\n",
      "\u001b[1;31m[I 2025-03-18 20:38:41.092 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(notebook_dir)\n",
    "\n",
    "\n",
    "\n",
    "model = \"AR\"\n",
    "sample = \"22Rv1\"\n",
    "project_path = \"/data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding\"\n",
    "jaspar_file = f\"/data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/motifs/{model}.jaspar\" \n",
    "# jaspar_file = \"/data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/motif.jaspar\"  # Update this path\n",
    "\n",
    "\n",
    "df = pl.read_parquet(project_path + \"/data/processed_results/\" + model + \"_\" + sample + \"_processed.parquet\", columns=[\"chr_name\", \"start\", \"end\", \"cell_line\", \"targets\", \"predicted\", \"weights\", \"probabilities\", \"attributions\"])\n",
    "df = df.rename({\"chr_name\": \"chr\"})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from src.utils.generate_training_peaks import run_bedtools_command\n",
    "def intersect_bed_files(main_df: pl.DataFrame, intersect_df: pl.DataFrame, region_type: str = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Intersect two BED files using bedtools and return the original DataFrame with overlap flags.\n",
    "    \n",
    "    Args:\n",
    "        main_df: Primary Polars DataFrame with BED data\n",
    "        intersect_df: Secondary Polars DataFrame to intersect with\n",
    "        region_type: Optional region type label to add to results\n",
    "        \n",
    "    Returns:\n",
    "        Original DataFrame with additional column indicating overlaps\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode='w') as main_file, \\\n",
    "         tempfile.NamedTemporaryFile(delete=False, mode='w') as intersect_file, \\\n",
    "         tempfile.NamedTemporaryFile(delete=False, mode='w') as result_file:\n",
    "        \n",
    "        main_path = main_file.name\n",
    "        intersect_path = intersect_file.name\n",
    "        result_path = result_file.name\n",
    "\n",
    "        # Write DataFrames to temporary files\n",
    "        main_df.write_csv(main_path, separator=\"\\t\", include_header=False)\n",
    "        intersect_df.write_csv(intersect_path, separator=\"\\t\", include_header=False)\n",
    "\n",
    "        # Run bedtools intersect with -c flag to count overlaps\n",
    "        command = f\"bedtools intersect -a {main_path} -b {intersect_path} -c > {result_path}\"\n",
    "        run_bedtools_command(command)\n",
    "\n",
    "        # Read results back into Polars DataFrame\n",
    "        result_df = pl.read_csv(\n",
    "            result_path,\n",
    "            separator=\"\\t\",\n",
    "            has_header=False,\n",
    "            new_columns=[*main_df.columns, \"overlap_count\"]\n",
    "        )\n",
    "\n",
    "    # Clean up temporary files\n",
    "    os.remove(main_path)\n",
    "    os.remove(intersect_path) \n",
    "    os.remove(result_path)\n",
    "\n",
    "    # Add boolean overlap column\n",
    "    result_df = result_df.with_columns(\n",
    "        pl.col(\"overlap_count\").gt(0).alias(\"overlaps_ground_truth\")\n",
    "    ).drop(\"overlap_count\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "ground_truth_file = \"/data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/data/transcription_factors/AR/merged/22RV1_AR_merged.bed\"\n",
    "\n",
    "\n",
    "df_ground_truth = pl.read_csv(ground_truth_file, \n",
    "                             separator=\"\\t\", \n",
    "                             has_header=False,\n",
    "                             new_columns=[\"chr\", \"start\", \"end\", \"weights\"],\n",
    "                             columns=[0,1,2,3])\n",
    "\n",
    "df_ground_truth_filtered = df_ground_truth.filter(pl.col(\"weights\") > pl.col(\"weights\").median()).drop(\"weights\")\n",
    "\n",
    "intersected_df = intersect_bed_files(df[[\"chr\", \"start\", \"end\"]], df_ground_truth_filtered)\n",
    "\n",
    "\n",
    "# add overlaps ground truth to df from intersected_df\n",
    "ground_truth_df = df.join(intersected_df, on=[\"chr\", \"start\", \"end\"], how=\"left\")\n",
    "# add overlaps_ground_truth to df under targets, 1 if overlaps_ground_truth is true, 0 otherwise\n",
    "ground_truth_df = ground_truth_df.with_columns(pl.when(pl.col(\"overlaps_ground_truth\")).then(1).otherwise(0).alias(\"targets\"))\n",
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all the targets that are 1, and an equal number of targets that are 0\n",
    "df_positive = ground_truth_df.filter(pl.col(\"targets\") == 1)\n",
    "df_negative = ground_truth_df.filter(pl.col(\"targets\") == 0).sample(n=len(df_positive), seed=42)\n",
    "df_balanced = pl.concat([df_positive, df_negative])\n",
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_attributions(df):\n",
    "    # Convert to numpy array first\n",
    "    attributions = df['attributions'].values\n",
    "    \n",
    "    # Create empty array with correct shape\n",
    "    n_samples = len(df)  # 46218\n",
    "    reshaped = np.empty((n_samples, 4096, 5))\n",
    "    \n",
    "    # Fill the array by properly accessing each (5,) array\n",
    "    for i, row in enumerate(attributions):\n",
    "        for j, element in enumerate(row):\n",
    "            reshaped[i, j] = element\n",
    "\n",
    "    # Split into ACGT and ATAC components\n",
    "    attrs_list = reshaped[..., :4].transpose(0, 2, 1)  # Shape: (n_samples, 4, 4096)\n",
    "    atac_list = reshaped[..., 4]  # Shape: (n_samples, 4096)\n",
    "\n",
    "    # Add chr, start, and end information to the ATAC component\n",
    "    chr_data = df['chr'].values\n",
    "    start_data = df['start'].values\n",
    "    end_data = df['end'].values\n",
    "    \n",
    "    # Expand the ATAC list to include the chr, start, and end information for each row\n",
    "    atac_list_with_meta = np.column_stack((chr_data, start_data, end_data, atac_list))\n",
    "        \n",
    "    return attrs_list, atac_list_with_meta\n",
    "\n",
    "# Usage:\n",
    "attrs_list, atac_list_with_meta = reshape_attributions(df_positive.to_pandas())\n",
    "print(f\"Attrs shape: {attrs_list.shape}\")\n",
    "#print(f\"ATAC shape: {atac_list.shape}\")\n",
    "print(f\"ATAT with meta shape: {atac_list_with_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_style('whitegrid')\n",
    "from tangermeme.plot import plot_logo\n",
    "from tangermeme.seqlet import recursive_seqlets\n",
    "\n",
    "# Get seqlets\n",
    "def get_seqlets(attrs_list):\n",
    "    attrs_array = np.stack(attrs_list, axis=0)\n",
    "    seqlets = recursive_seqlets(attrs_array.sum(axis=1))\n",
    "    \n",
    "    nt_idx = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    \n",
    "    # Add sequences to seqlets df\n",
    "    sequences = []\n",
    "    for i in range(len(seqlets)):\n",
    "        sample = seqlets.iloc[i]\n",
    "        start = int(sample['start'])\n",
    "        end = int(sample['end'])\n",
    "        sample_idx = int(sample['example_idx'])\n",
    "        \n",
    "        sample_attrs = attrs_array[sample_idx, :, start:end].T.squeeze()\n",
    "        hits = np.argmax(sample_attrs, axis=1)\n",
    "        seq = ''.join([nt_idx[i] for i in hits])\n",
    "        sequences.append(seq)\n",
    "    seqlets['sequence'] = sequences\n",
    "    \n",
    "    return seqlets\n",
    "\n",
    "# Plot function (simplified version)\n",
    "def plot_seqlet(seqlets, attrs_list, sample_rank=0, context_size=20):\n",
    "    sample = seqlets.iloc[[sample_rank]]\n",
    "    slice = int(sample['example_idx'].tolist()[0])\n",
    "    sequence = sample['sequence'].tolist()[0]\n",
    "    start = int(sample['start'].tolist()[0])\n",
    "    end = int(sample['end'].tolist()[0])\n",
    "    \n",
    "    seqlen = end - start\n",
    "    window_size = seqlen + (context_size * 2)\n",
    "    \n",
    "    X_attr = attrs_list[slice]\n",
    "    X_attr = X_attr.astype(np.float64)\n",
    "    \n",
    "    TSS_pos = int(np.mean([start, end]))\n",
    "    window = (TSS_pos - (window_size // 2), TSS_pos + (window_size // 2))\n",
    "    \n",
    "    plt.figure(figsize=(16, 9), dpi=300)\n",
    "    ax = plt.subplot(111)\n",
    "    plot_logo(\n",
    "        X_attr,\n",
    "        ax=ax,\n",
    "        start=window[0],\n",
    "        end=window[1]\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(\"Genomic Coordinate\")\n",
    "    plt.ylabel(\"Attributions\")\n",
    "    plt.title(f\"DeepLIFT Attributions for sample: {slice} | {sequence}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlets = get_seqlets(attrs_list)\n",
    "seqlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seqlet(seqlets, attrs_list, sample_rank=6127, context_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class PWM:\n",
    "    \"\"\"Container for position weight matrix data.\"\"\"\n",
    "    name: str\n",
    "    matrix: np.ndarray\n",
    "    bases: List[str] = field(default_factory=lambda: ['A', 'C', 'G', 'T'])\n",
    "    \n",
    "    def get_consensus(self, prob_threshold: float = 0.25) -> str:\n",
    "        \"\"\"Get consensus sequence from PWM using IUPAC ambiguity codes.\"\"\"\n",
    "        iupac_map = {\n",
    "            'A': 'A', 'C': 'C', 'G': 'G', 'T': 'T',\n",
    "            'AC': 'M', 'AG': 'R', 'AT': 'W',\n",
    "            'CG': 'S', 'CT': 'Y', 'GT': 'K',\n",
    "            'ACG': 'V', 'ACT': 'H', 'AGT': 'D', 'CGT': 'B',\n",
    "            'ACGT': 'N'\n",
    "        }\n",
    "        \n",
    "        consensus = []\n",
    "        for pos_probs in self.matrix.T:\n",
    "            significant_bases = ''.join(b for b, p in zip(self.bases, pos_probs) \n",
    "                                     if p >= prob_threshold)\n",
    "            significant_bases = ''.join(sorted(significant_bases))\n",
    "            consensus.append(iupac_map.get(significant_bases, 'N'))\n",
    "        return ''.join(consensus)\n",
    "\n",
    "def parse_jaspar(jaspar_file: str) -> PWM:\n",
    "    \"\"\"Parse a JASPAR format PWM file.\"\"\"\n",
    "    with open(jaspar_file) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if not lines or len(lines) != 5:\n",
    "        raise ValueError(\"Invalid JASPAR format\")\n",
    "        \n",
    "    name = lines[0].split()[0]\n",
    "    matrix = []\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        nums = line.split('[')[1].split(']')[0].strip().split()\n",
    "        matrix.append([float(x) for x in nums])\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    matrix = matrix / matrix.sum(axis=0)\n",
    "    \n",
    "    return PWM(name=name, matrix=matrix)\n",
    "\n",
    "pwm = parse_jaspar(jaspar_file)\n",
    "print(f\"Loaded PWM: {pwm.name}\")\n",
    "print(f\"Consensus sequence: {pwm.get_consensus()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iupac_match(a: str, b: str) -> bool:\n",
    "    \"\"\"Check if two IUPAC nucleotide codes match.\"\"\"\n",
    "    iupac = {\n",
    "        'A': {'A'},\n",
    "        'C': {'C'},\n",
    "        'G': {'G'},\n",
    "        'T': {'T'},\n",
    "        'R': {'A', 'G'},\n",
    "        'Y': {'C', 'T'},\n",
    "        'S': {'G', 'C'},\n",
    "        'W': {'A', 'T'},\n",
    "        'K': {'G', 'T'},\n",
    "        'M': {'A', 'C'},\n",
    "        'B': {'C', 'G', 'T'},\n",
    "        'D': {'A', 'G', 'T'},\n",
    "        'H': {'A', 'C', 'T'},\n",
    "        'V': {'A', 'C', 'G'},\n",
    "        'N': {'A', 'C', 'G', 'T'}\n",
    "    }\n",
    "    \n",
    "    a = a.upper()\n",
    "    b = b.upper()\n",
    "    \n",
    "    if a not in iupac or b not in iupac:\n",
    "        raise ValueError(f\"Invalid IUPAC code: {a if a not in iupac else b}\")\n",
    "        \n",
    "    return bool(iupac[a] & iupac[b])\n",
    "\n",
    "\n",
    "def levenshtein_iupac(seq1: str, seq2: str) -> int:\n",
    "    \"\"\"Calculate Levenshtein distance between two DNA sequences with IUPAC codes.\"\"\"\n",
    "    if not seq1: return len(seq2)\n",
    "    if not seq2: return len(seq1)\n",
    "    \n",
    "    # Initialize lists instead of range objects\n",
    "    previous_row = list(range(len(seq2) + 1))\n",
    "    current_row = [0] * (len(seq2) + 1)\n",
    "    \n",
    "    for i, c1 in enumerate(seq1):\n",
    "        current_row[0] = i + 1\n",
    "        \n",
    "        for j, c2 in enumerate(seq2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (not iupac_match(c1, c2))\n",
    "            \n",
    "            current_row[j + 1] = min(insertions, deletions, substitutions)\n",
    "            \n",
    "        previous_row, current_row = current_row, [0] * (len(seq2) + 1)  # Reset current_row\n",
    "        \n",
    "    return previous_row[-1]\n",
    "\n",
    "\n",
    "def score_seqlet(pwm: PWM, seq: str) -> Tuple[float, int]:\n",
    "    \"\"\"Score a sequence against a PWM using IUPAC-aware Levenshtein distance.\"\"\"\n",
    "    from Bio import motifs\n",
    "    from Bio.Seq import Seq\n",
    "    \n",
    "    seq_len = len(seq)\n",
    "    pwm_width = pwm.matrix.shape[1]\n",
    "    consensus = pwm.get_consensus()\n",
    "    \n",
    "    if seq_len < pwm_width:\n",
    "        max_score = float('-inf')\n",
    "        best_pos = 0\n",
    "        for i in range(pwm_width - seq_len + 1):\n",
    "            cons_slice = consensus[i:i+seq_len]\n",
    "            raw_dist = levenshtein_iupac(seq, cons_slice)\n",
    "            norm_score = 1 - (raw_dist / max(len(seq), len(cons_slice)))\n",
    "            if norm_score > max_score:\n",
    "                max_score = norm_score\n",
    "                best_pos = i\n",
    "        return max_score, best_pos\n",
    "    \n",
    "    elif seq_len == pwm_width:\n",
    "        raw_dist = levenshtein_iupac(seq, consensus)\n",
    "        norm_score = 1 - (raw_dist / len(consensus))\n",
    "        return norm_score, 0\n",
    "    \n",
    "    else:\n",
    "        max_score = float('-inf')\n",
    "        best_pos = 0\n",
    "        for i in range(seq_len - pwm_width + 1):\n",
    "            subseq = seq[i:i+pwm_width]\n",
    "            raw_dist = levenshtein_iupac(subseq, consensus)\n",
    "            norm_score = 1 - (raw_dist / len(consensus))\n",
    "            if norm_score > max_score:\n",
    "                max_score = norm_score\n",
    "                best_pos = i\n",
    "        return max_score, best_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "scores = []\n",
    "positions = []\n",
    "for _, row in tqdm(seqlets.iterrows(), total=len(seqlets)):\n",
    "    score, pos = score_seqlet(pwm, row['sequence'])\n",
    "    scores.append(score)\n",
    "    positions.append(pos)\n",
    "\n",
    "# Add scores to dataframe\n",
    "seqlets['pwm_score'] = scores\n",
    "seqlets['pwm_position'] = positions\n",
    "\n",
    "# Sort by score and display top matches\n",
    "top_matches = seqlets.sort_values('pwm_score', ascending=False).head(10)\n",
    "print(\"\\nTop 10 PWM matches:\")\n",
    "print(top_matches[['sequence', 'pwm_score', 'pwm_position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_fasta(sequences, outfile):\n",
    "    \"\"\"Write sequences to FASTA format with auto-generated headers.\"\"\"\n",
    "    with open(outfile, 'w') as f:\n",
    "        for i, seq in enumerate(sequences):\n",
    "            f.write(f'>seq_{i+1}\\n{seq}\\n')\n",
    "\n",
    "\n",
    "def save_seqlets(seqlets, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    pos_seqlets = seqlets[seqlets['attribution'] > 0].reset_index(drop=True)\n",
    "    neg_seqlets = seqlets[seqlets['attribution'] < 0].reset_index(drop=True)\n",
    "\n",
    "    pos_seqlets.to_csv(os.path.join(output_dir, \"positive_seqlets.csv\"), \n",
    "                       index=False)\n",
    "    neg_seqlets.to_csv(os.path.join(output_dir, \"negative_seqlets.csv\"), \n",
    "                       index=False)\n",
    "    \n",
    "    candidate_motifs = pos_seqlets['sequence'].tolist()\n",
    "    write_fasta(\n",
    "        candidate_motifs, \n",
    "        outfile=os.path.join(output_dir, \"positive_seqlets.fa\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "save_seqlets(seqlets, f\"/data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/output/{model}_{sample}/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run python levenstein.py --jaspar motif.jaspar --seqlets positive_seqlets.csv --output lev_pwm.csv\n",
    "os.system(f\"python /data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/levenstein.py --jaspar /data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/motif.jaspar --seqlets /data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/output/{model}_{sample}/positive_seqlets.csv --output /data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/output/{model}_{sample}/lev_pwm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_seqlet = 5\n",
    "receptor_name = model.split(\"_\")[0]\n",
    "dir = f\"/data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/output/{model}_{sample}\"\n",
    "os.system(f\"Rscript /data1/datasets_1/human_cistrome/chip-atlas/peak_calls/tfbinding_scripts/tf-binding/src/inference/interpretability/posthoc.R {min_seqlet} {receptor_name} {dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to plot atac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting contribution from ATAC\n",
    "#atac_list\n",
    "print(atac_list_with_meta[:500])\n",
    "plt.plot(atac_list_with_meta[:500])\n",
    "plt.xlabel(\"attribution index\")\n",
    "plt.ylabel(\"attribution score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(atac_list[10])\n",
    "plt.title(\"atac index 10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seqlet(seqlets, attrs_list, atac_list_with_meta, sample_rank=0, context_size=20):\n",
    "    # Get the seqlet of interest\n",
    "    sample = seqlets.iloc[[sample_rank]]\n",
    "    slice = int(sample['example_idx'].tolist()[0])  # Get the example_idx\n",
    "    sequence = sample['sequence'].tolist()[0]\n",
    "    start = int(sample['start'].tolist()[0])\n",
    "    end = int(sample['end'].tolist()[0])\n",
    "    \n",
    "    seqlen = end - start\n",
    "    window_size = seqlen + (context_size * 2)\n",
    "    \n",
    "    X_attr = attrs_list[slice]\n",
    "    X_attr = X_attr.astype(np.float64)\n",
    "    \n",
    "    TSS_pos = int(np.mean([start, end]))\n",
    "    window = (TSS_pos - (window_size // 2), TSS_pos + (window_size // 2))\n",
    "    \n",
    "    # Extract ATAC data for the same region based on example_idx (slice)\n",
    "    atac_data = atac_list_with_meta[slice]\n",
    "    atac_chr = atac_data[0]  # Chromosome\n",
    "    atac_start = atac_data[1]  # Start\n",
    "    atac_end = atac_data[2]  # End\n",
    "    atac_values = atac_data[3:]  # The actual ATAC contributions\n",
    "    \n",
    "    # Adjust ATAC values to match the region of interest (start to end)\n",
    "    atac_region = atac_values[start:end]\n",
    "    \n",
    "    # Ensure that ATAC values are numeric and replace NaN or non-numeric values with 0\n",
    "    atac_region = np.nan_to_num(atac_region, nan=0.0)  # Replace NaN with 0\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(16, 9), dpi=300, sharex=True)\n",
    "    \n",
    "    # Plot the motif logo\n",
    "    ax1 = axs[0]\n",
    "    plot_logo(\n",
    "        X_attr,\n",
    "        ax=ax1,\n",
    "        start=window[0],\n",
    "        end=window[1]\n",
    "    )\n",
    "    ax1.set_ylabel(\"Attributions\")\n",
    "    ax1.set_title(f\"DeepLIFT Attributions for sample: {slice} | {sequence}\")\n",
    "    \n",
    "    # Plot the ATAC contributions\n",
    "    ax2 = axs[1]\n",
    "    ax2.plot(range(start, end), atac_region, color='b', label=\"ATAC Contributions\")\n",
    "    ax2.fill_between(range(start, end), atac_region, color='b', alpha=0.2)\n",
    "    ax2.set_xlabel(\"Genomic Coordinate\")\n",
    "    ax2.set_ylabel(\"ATAC Contributions\")\n",
    "    ax2.set_title(f\"ATAC Contributions for {atac_chr} | {atac_start}-{atac_end}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq(seqlets, attrs_list, atac_list_with_meta, sample_rank=6127, context_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in chr, start, end, \n",
    "\n",
    "import pandas as  pd\n",
    "# Plotting atac attributions with the x representing the genomic position\n",
    "genomic_position = []\n",
    "attribution_score = []\n",
    "\n",
    "plot_df = pd.DataFrame({})\n",
    "\n",
    "for index in attrs_list:\n",
    "    for pos, attribution in enumerate(index):\n",
    "        genomic_position.append(pos)\n",
    "        attribution_score.append(attribution)\n",
    "\n",
    "plot_df = pd.DataFrame({\n",
    "    'Genomic Position': genomic_position,\n",
    "    'Attribution Score': attribution_score\n",
    "})\n",
    "\n",
    "print(plot_df)\n",
    "\n",
    "#plt.plot(plot_df['Genomic Position'], plot_df['Attribution Score'], marker='o')\n",
    "#plt.title('Genomic Position vs Attribution Score')\n",
    "#plt.xlabel('Genomic Position')\n",
    "#plt.ylabel('Attribution Score')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# .dt.list.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to just adapt plotting function\n",
    "\n",
    "# Genomic position of atac data\n",
    "# PILEUP PROCESSING\n",
    "import pysam\n",
    "from pathlib import Path\n",
    "\n",
    "pileup_dir = Path(\"/data1/projects/human_cistrome/aligned_chip_data/merged_cell_lines/22Rv1/pileup_mod/\")\n",
    "def process_pileups(pileup_dir: Path, chr_name: str, start: int, end: int):\n",
    "    context_length = 4_096\n",
    "    interval_length = end - start\n",
    "    extra_seq = context_length - interval_length\n",
    "    extra_left_seq = extra_seq // 2\n",
    "    extra_right_seq = extra_seq - extra_left_seq\n",
    "\n",
    "    start -= extra_left_seq\n",
    "    end += extra_right_seq\n",
    "\n",
    "    # get the pileup file for the given chromosome\n",
    "    pileup_file = pileup_dir / f\"{chr_name}.pileup.gz\"\n",
    "\n",
    "    assert pileup_file.exists(), f\"pileup file for {pileup_file} does not exist\"\n",
    "\n",
    "    tabixfile = pysam.TabixFile(str(pileup_file))\n",
    "\n",
    "    records = []\n",
    "    for rec in tabixfile.fetch(chr_name, start, end):\n",
    "        records.append(rec.split(\"\\t\"))\n",
    "\n",
    "    # Convert records to a DataFrame using Polars:\n",
    "    df = pl.DataFrame(\n",
    "        {\n",
    "            \"chr_name\": [rec[0] for rec in records],\n",
    "            \"position\": [int(rec[1]) for rec in records],\n",
    "            \"nucleotide\": [rec[2] for rec in records],\n",
    "            \"count\": [float(rec[3]) for rec in records],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot function (simplified version)\n",
    "def plot_seqlet(seqlets, attrs_list, sample_rank=0, context_size=20):\n",
    "    sample = seqlets.iloc[[sample_rank]]\n",
    "    slice = int(sample['example_idx'].tolist()[0])\n",
    "    sequence = sample['sequence'].tolist()[0]\n",
    "    start = int(sample['start'].tolist()[0])\n",
    "    end = int(sample['end'].tolist()[0])\n",
    "    \n",
    "    seqlen = end - start\n",
    "    window_size = seqlen + (context_size * 2)\n",
    "    \n",
    "    X_attr = attrs_list[slice]\n",
    "    X_attr = X_attr.astype(np.float64)\n",
    "    \n",
    "    TSS_pos = int(np.mean([start, end]))\n",
    "    window = (TSS_pos - (window_size // 2), TSS_pos + (window_size // 2))\n",
    "    \n",
    "    plt.figure(figsize=(16, 9), dpi=300)\n",
    "    ax = plt.subplot(111)\n",
    "    plot_logo(\n",
    "        X_attr,\n",
    "        ax=ax,\n",
    "        start=window[0],\n",
    "        end=window[1]\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(\"Genomic Coordinate\")\n",
    "    plt.ylabel(\"Attributions\")\n",
    "    plt.title(f\"DeepLIFT Attributions for sample: {slice} | {sequence}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
